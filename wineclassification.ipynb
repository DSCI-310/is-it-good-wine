{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c266b9a-29f2-46bb-9e68-f2cf9031f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from src.pipeline import pipe_build\n",
    "from src.optimize import hp_optimizer\n",
    "from src.preprocess import preprocessor\n",
    "from src.grapher import correlation_table, bar_chart, vis_tree, compare_scores, show_coefficients, show_correct\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb875da-2b90-445e-ab7f-2409a93f0c5f",
   "metadata": {},
   "source": [
    "# Alcohol Non-Anonymous: What chemical ingredients contribute to better rated wine?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac7d3f-fecd-43e3-8c44-577846c6596e",
   "metadata": {},
   "source": [
    "DSCI 310 Project Group 11\n",
    "\n",
    "By: Anthony Huang, Justin Kung, and Christina Pan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403a703-826a-401b-9ea1-fc78b26bb0a9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377e50f-dab0-405f-ad9c-5412ea5afe6f",
   "metadata": {},
   "source": [
    "As one of the most popular drinks in the world,  wine has historically and continues to hold an important role in society. According to Ritchie and Roser, the consumption of wine is growing significantly since 1960.  Specifically, Canadian on average consumed  0.3 liters of wine in 1964, and the consumption grows to 2.1 liters of wine in 2019 (Ritchie & Roser, 2019). As the demand of wine grows rapidly, more and more wine companies begin to focus on improve the wine quality in order to attracts consumers and make more profit. However, due to complexity of wine production process, a sophisticated  way to predict wine quality is still rare. Furthermore, recent research done by Filipe-Ribeiro et. al (2017) suggested that certain chemical ingredients are found correlated to wine quality. **Therefore, this project will examine the relationship between chemical ingredients and the quality of wine, and attempted to answer the question of whether wine can be classified as 'good' or 'bad' based on its chemical ingredients.** \n",
    "\n",
    "This project will use the red wine dataset collected from UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/wine+quality). This dataset contains 1599 observations, and 12 variables shown as follow:  \n",
    ">1 - fixed acidity<br>2 - volatile acidity<br>3 - citric acid<br>4 - residual sugar\n",
    "<br>5 - chlorides\n",
    "<br>6 - free sulfur dioxide\n",
    "<br>7 - total sulfur dioxide\n",
    "<br>8 - density\n",
    "<br>9 - pH\n",
    "<br>10 - sulphates\n",
    "<br>11 - alcohol\n",
    "<br>12 - quality (score between 0 and 10)\n",
    "\n",
    "For this project, wine quality will be used as the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ded69-f27b-4d94-890d-31fbe37feb83",
   "metadata": {},
   "source": [
    "### Preliminary EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9779035-389c-44c3-9809-5ca41e96268a",
   "metadata": {},
   "source": [
    "![title](results/eda_dfprev.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18adcd0c-64a9-4afa-aa60-52e4c964d9e1",
   "metadata": {},
   "source": [
    "After loading in the data set from the UCI Machine Learning Database, we first visualize the distribution *(Figure 1)* of wine qualities via histogram. Viewing the distribution will affect where the split in our data occurs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9f6a8d6-95ec-48f1-a4f0-321f2ace33d4",
   "metadata": {},
   "source": [
    "![title](results/eda_barcount.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df12b0a-f195-431c-9eb9-6ef7172fc1a0",
   "metadata": {},
   "source": [
    "*Figure 1: quality vs. count(records) of our wine dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71b9c9-87cc-4b9e-b884-562240cf8d94",
   "metadata": {},
   "source": [
    "**Judging from Figure 2, we have a right-skewed distribution with a mode at quality = 5. Therefore, we should classify 'good' wines as any wine rated at 6, 7 or 8. Consequently, 'bad' wines will be rated at 3, 4, and 5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebba7c-938c-4b7c-b2b9-1187ad050752",
   "metadata": {},
   "source": [
    "We then visualize the correlation grid between all of our feature variables. This assists to identify patterns and relationships between the variables, allowing us to gain insights into the underlying structure of the data and potentially identify features that are highly correlated with the target variable. This can help us to better understand the data and inform our feature selection and model building decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afa934-238f-4ccb-b57a-1f71a2081721",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](results/eda_corrtab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799bb2c7-e180-4f39-bc8e-056313af1149",
   "metadata": {},
   "source": [
    "*Figure 2: Correlation grid of all available features within the wine dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae16f99-08e3-4ad8-8aca-f022fbbca21f",
   "metadata": {},
   "source": [
    "From figure 2, we can see that acids, densities, and sulfur dioxides tend to rise together, whereas acid and pH levels posess an inverse relationship. Furthermore, the 'quality' rating variable has the highest correlation with alcohol content - and the highest negative correlation with volatile acidity.\n",
    "\n",
    "This correlation grid is based off of un-scaled values which may unfairly adjust the coefficients of these relationships. We will explore these coefficients properly in the 'Methods' section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1c8aa-4067-4fc5-a309-f60990bf4952",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc386e36-ae89-47bc-9f08-d9f752fcc2ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Overview of Methodology**\n",
    "\n",
    "In machine learning, model selection plays a critical role in achieving the best performance on a given task. As our task involves classifying wines as 'good' or 'bad' based on their chemical properties, we will proceed with the four classification models to predict the outcomes of our wine data set. to predict the outcomes of our wine dataset: \n",
    "\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Naive Bayes\n",
    "- SVM with RBF kernel \n",
    "\n",
    "The outcomes of these models (i.e. each model's accuracy in classification) will assist us in determining whether our 11 feature values (subtracting the 'quality' metric) can be accurate predictors for classifying wines as 'good' or 'bad', or if there are other factors at play. A baseline model will also be included as a point of comparison for all the other machine learning models used.\n",
    "\n",
    "In our quality classification, the input features are chemical composition of wine samples: such as alcohol content, fixed acidity, chlorides, sulphates, etc. The output label is derived from the *quality* rating assigned to each wine sample (sourced from various wine critics). If the quality score greater than 5, it is classified as 'good' - whereas below 5 results in a 'bad' classification.\n",
    "\n",
    "Additionally, we split the data into separate train (0.80) and test set (0.20) to help us evaluate the performance of a model. We utilize the training data to expose the model to the dataset, and test the model's ability to predict the 'target' value of our test data. This provides an estimate of how well the model will generalize to data that we have not seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c384d-cfd8-460d-afad-161bf6807470",
   "metadata": {},
   "source": [
    "Checking for class imbalance in our dataset *(Figure 3)* is essential before doing classification because it affects the performance of classification algorithms. If the dataset is imbalanced, where one class has significantly more or fewer samples than the other, the algorithm may have a bias towards the majority class, leading to poor performance on the minority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c39483a4-e973-418c-86b2-8ab3f089b9e5",
   "metadata": {},
   "source": [
    "![Class Imbalance](results/eda_dfclasses.png)\n",
    "\n",
    "*Figure 3: Value counts of positive and negative classes in our wine training dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58dcb8-c9bc-4ce6-9c3d-4088911ab4b4",
   "metadata": {},
   "source": [
    "As the ratio between positive classes (good - 1) and negative classes (bad - 0), our training set has an adequate level of class balance, so re-balancing is unnecessary. We then separate our train and test sets into 'X' (inputs) and 'y' (outputs). We will then use our 'X' variables to train our model to predict the correct 'y' value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d68ac-d336-43d7-94a2-b6c2a60e1479",
   "metadata": {},
   "source": [
    "### 0. Baseline Model\n",
    "\n",
    "To start, we will be using sklearn's DummyClassifier() to make a baseline model. The model will simply predict every instance as the most commonly occurring target value. In this case, it will predict the target value of every instance to be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8f56e-130c-44cf-8f62-1cb73ca09c57",
   "metadata": {},
   "source": [
    "![title](results/dummyreport.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990522e-ed45-4cdb-8e05-cce58da497ce",
   "metadata": {},
   "source": [
    "From this, as long as the accuracy of the following models give an accuracy score of greater than 0.5167, they will be performing better than a baseline model that essentially guesses at random."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a6cc5f0-dd6f-4166-b842-808fec11ee9f",
   "metadata": {},
   "source": [
    "![title](results/dummycorrect.png)\n",
    "\n",
    "*Figure 4: Baseline model TP/TN (Correct) versus FP/FN (Incorrect) predictions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320b4b5-164d-4c52-bf8d-0d83477cd3fd",
   "metadata": {},
   "source": [
    "The code above displays how many guesses our classifier got right (true positive/true negative classifications) versus how many it got wrong (false positives/false negative classifications). As you can see, the dummy classifier was only able to classify 248 wine samples correctly, while 232 wine samples were classified incorrectly on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93cf6ca-aef7-4b28-904b-023644e9b659",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb91ceb-ba72-40f2-acf8-250dac9c73a9",
   "metadata": {},
   "source": [
    "*For report simplicity: the methods section of our Logistic Regression will go into detail for each step, whereas our remaining models will provide explanations for additional steps that were not performed in Logistic Regression.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LogisticRegression() is an sklearn algorithm that models the probability of a binary output label given a set of input features. Given a set of $n$ input examples, each with $m$ features, the goal of logistic regression is to learn a set of weights $\\mathbf{w} = [w_0, w_1, ..., w_m]$ that can be used to predict the probability of a binary outcome $y$ (where $y \\in {0, 1}$) for any input example $\\mathbf{x} = [x_1, x_2, ..., x_m]$. The logistic regression model uses a sigmoid function to map the output of the linear regression model to a probability between 0 and 1. The sigmoid function is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "where $z$ is the output of the linear regression model:\n",
    "\n",
    "\\begin{equation}\n",
    "z = w_0 + w_1 x_1 + w_2 x_2 + ... + w_m x_m\n",
    "\\end{equation}\n",
    "\n",
    "The output of the logistic regression model is then given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma(z)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}$ is the predicted probability of the binary outcome $y$.\n",
    "\n",
    "**In our wine quality classification, we use logistic regression to classify wines as either good (positive class) or bad (negative class) based on their chemical properties. We will use a dataset containing $1599$ examples of wines, each with $12$ features.The result of the logistic function will classify our wine as 'good' or 'bad'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8d663-5d43-45ac-9adf-8203c383ac60",
   "metadata": {},
   "source": [
    "##### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fde34c-35b4-4d98-97ed-07162ce6cd84",
   "metadata": {},
   "source": [
    "Hyperparameter optimization is the process of finding the optimal values for the 'knobs' of an algorithm. Properly tuning these hyperparameters help us improve the performance of the model - therefore, we will perform this same optimization on all four of our models. \n",
    "\n",
    "For the case of our logistic regression is the regularization parameter 'C'. The regularization parameter 'C' controls the amount of regularization applied to the model, and it helps prevent overfitting by adding a penalty to incorrect classifications. We generated a range of potential values of C between 0.0001 and 100000 and measured their mean cross_validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc8f57-85c9-477b-90c9-11c07917f6b4",
   "metadata": {},
   "source": [
    "![title](results/lrtable.png)\n",
    "\n",
    "*Figure 5: Logistic Regression Hyperparameter Optimization*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304b54c-5d0e-4d5c-894b-fcd223c834c2",
   "metadata": {},
   "source": [
    "Based on our mean cross-validation scores for each possible value of C, we will choose C = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17912ff-920d-4f47-a9df-e34a68e57676",
   "metadata": {},
   "source": [
    "##### Applying the LogisticRegression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5719b0-2c8f-44de-bb84-c60f4bbafaca",
   "metadata": {},
   "source": [
    "Before we proceed with building the LogisticRegression model, we scale our data before applying a LogisticRegression model to ensure that all the features have the same importance in the model. LogisticRegression assumes that the features are normally distributed and are on the same scale. Thus, failure to scale our data beforehand may lead to some features may dominate the others in the model.\n",
    "\n",
    "Scaling the data also assists in interpreting the coefficients of the model. The coefficients represent the effect of each feature on the target variable, and scaling the features ensures that the coefficients are comparable and meaningful. For example, if the features are on different scales, then the coefficients cannot be compared directly, and their magnitudes do not reflect the actual effect of the features on the target variable.\n",
    "\n",
    "To do this, we instantiate a pipeline object that applies prerequisite scaling, and then fits the training data to our Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183817a0-f1ad-462c-a365-d8bb1ae9f366",
   "metadata": {},
   "source": [
    "![title](results/coeftable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87fedf-a9d5-4d59-9b48-4e317e51907a",
   "metadata": {},
   "source": [
    "The variables with most influence per unit are volatile acidity and alcohol. Volatile acidity has a negative relationship with quality whereas alcohol has a positive relationship with quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacd9d5c-6f95-44cb-a8a3-f9733aa0f249",
   "metadata": {},
   "source": [
    "![title](results/lrcorrect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e35b73-c653-47ba-b4b8-6f5033584278",
   "metadata": {},
   "source": [
    "Our model has produced an accuracy score of 72%; correctly classifying 72% of the test dataset's wine samples as either 'good' or 'bad', indicating a moderate level of predictive power. Furthermore, comparing this metric to our baseline model's accuracy of 52% - it is clear that the regression model is succeeding in some capacity. We can see the exact numbers of how many data points were correctly classified versus not correctly classified above.\n",
    "\n",
    "However, it is important to note that the accuracy score alone may not be sufficient to evaluate the performance of the model. Therefore, we produce a classification report containing precision, recall, and F1-score in order to get a better understanding of the model's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450e753-12d4-4256-be9b-fa9594f2a609",
   "metadata": {},
   "source": [
    "![title](results/lrreport.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92faaa-e9ef-49c7-b317-132f3640cdd5",
   "metadata": {},
   "source": [
    "### 2. SVC RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194ad66-272f-43e1-a111-10cf07c5de7f",
   "metadata": {},
   "source": [
    "SVM RBF fits a decision boundary that separates the positive and negative classes with the largest margin. The decision boundary is represented as a hyperplane in a high-dimensional feature space, and the RBF kernel is used to compute the distance between input examples and the decision boundary.\n",
    "\n",
    "The function is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are input examples, and $\\gamma$ is a hyperparameter that controls the shape of the kernel function.\n",
    "\n",
    "**SVM RBF (Support Vector Machine with Radial Basis Function) Classification may be an effective choice for our model due to its ability to model complex decision boundaries and capture non-linearity in the data. As our dataset contains 11 different features with possibly different relationships with our 'target' variable, utilizing SVM RBF may result in a better accuracy score than Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e86ba-9d73-489e-a972-e6a9f5ca9cad",
   "metadata": {},
   "source": [
    "![title](results/svmtable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273cf97-c05c-4a9d-a77f-f3315187b43a",
   "metadata": {},
   "source": [
    "We perform the same hyperparameter optimization we applied to our LogisticRegression model but we tune an additional hyperparameter for our SVM RBF model: gamma. This hyperparameter controls the shape of the decision boundary (which we will visualize later), adding complexity by raising its value.\n",
    "\n",
    "After optimizing hyperparameters, we fit and score the model similarly to our LogisticRegession process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0deec7b-d5de-41b1-9153-a1665ec5b857",
   "metadata": {},
   "source": [
    "![title](results/svmreport.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc2de3-d6d6-4b35-b3c2-638dfa06f8de",
   "metadata": {},
   "source": [
    "Utilizing SVC RBF led to a slightly higher accuracy score of 76% compared to our original Logistic Regression model, which suggests the model is marginally more confident when predicting 'good' and 'bad' wine. Both of these models performed significantly better than out baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a8a6e-7289-4a01-b85e-2b389b936106",
   "metadata": {},
   "source": [
    "![title](results/svmcorrect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dce2ac-339a-44c9-b022-0de1dfffb189",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35c232-3027-41ff-8824-9dad5208073d",
   "metadata": {},
   "source": [
    "When deciding between tree-model classifications, the two obvious options are **Decision Trees** and **Random Forest** classifiers. While Random Forest Classifiers are more complex and may achieve higher accuracy scores on large datasets, it may not be the best option for small datasets. Random Forest requires a large number of samples and features to achieve good performance. With a small dataset like our wine set, Random Forest may overfit the training data and fail to generalize well on unseen data. Moreover, the computational cost of training and testing a Random Forest can be much higher than a simple Decision Tree, especially with limited computational resources.\n",
    "\n",
    "Therefore, we chose a **Decision Tree Classifier** because of its easy interpretability and computational efficiency.\n",
    "\n",
    "In our wine classification, we use an algorithm that chooses which chemical splits the dataset with the lowest impurity (in laymen's terms: the ratio between correct and incorrect classes) at each step. We will use the Gini index as a measure of impurity, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Gini(D) = 1 - \\sum_{i=1}^k p_i^2\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ is a set of training examples, $k$ is the number of classes, and $p_i$ is the proportion of examples in class $i$.\n",
    "\n",
    "The algorithm continues recursively until a stopping criterion is met, such as when all examples in a subset belong to the same class, or when a maximum tree depth or minimum number of examples per leaf is reached. We will run cross-validation to optimize the maximum tree depth hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdedb7-9c46-4ac1-927b-5856a436c9ef",
   "metadata": {},
   "source": [
    "##### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885cd73-db64-4183-b58c-09930603e56f",
   "metadata": {},
   "source": [
    "![title](results/dtctable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff68a77-b6ce-400a-86b9-6e0ffb5b1894",
   "metadata": {},
   "source": [
    "##### Fitting and representing the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea254a-6e11-45fa-b3fa-6fc995c51f8b",
   "metadata": {},
   "source": [
    "According to our hyperparameter optimization, max_depth = 7 is producing the best classification model. For representation purposes Figure 4 is a decision tree with max_depth = 3 (plotting high max_depths is visually terrifying) that displays the first three decision parameters being used in our classifier. Alcohol, volatile acidity, total/free sulfur dioxide, and sulphates are our participating features. **However, the gini impurity of each node is alarming - which could suggest that the model is struggling to clearly split the dataset based on these features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d80fb9-56de-4e74-a326-dcd52b088129",
   "metadata": {},
   "source": [
    "![title](results/dtctree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b2f49-d1c1-4421-81a5-24f180dd0a70",
   "metadata": {},
   "source": [
    "*Figure 3: Decision Tree visualization at max_depth = 3. Highlight the first three decision parameters used in the decision tree.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478dc03-311a-4116-b924-2a440f213641",
   "metadata": {},
   "source": [
    "![title](results/dtcreport.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8edee-9765-4388-893e-2a2e806055d8",
   "metadata": {},
   "source": [
    "Our DecisionTreeClassifier performed marginally worse than our previous two models. This may be due to decision trees being based on simple if-then rules *(Figure 3)*, and they may not be able to model complex relationships between the features and the target variable. Our other models on the other hand are generally more flexible models - handling both linear and nonlinear relationships between our features and the 'target' variable. Additionally, our decision tree may be more susceptible to noise and outliers in the data, which can lead to spurious splits in the tree and a less accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691ddf9-3e5a-44d9-8da7-d50519b7d65d",
   "metadata": {},
   "source": [
    "![title](results/dtccorrect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f4524-ea2f-423b-9db8-72d826e578a6",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda0d95-b446-4a2e-97ed-7c016ccab8cf",
   "metadata": {},
   "source": [
    "Naive Bayes calculates the conditional probability of each class given a set of features by multiplying the probability of each feature given the class. The class with the highest probability is then assigned to the input data.\n",
    "\n",
    "To train the model, it first calculates the prior probability of each class based on the frequency of occurrence of the class in the training data. It then calculates the conditional probability of each feature given each class by counting the number of times a feature appears in the training data for each class.\n",
    "\n",
    "\\begin{equation}\n",
    "P(y \\mid x_1, x_2, \\dots, x_m) = \\frac{P(y) P(x_1, x_2, \\dots, x_m \\mid y)}{P(x_1, x_2, \\dots, x_m)}\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the class label, $x_1, x_2, \\dots, x_m$ are the wine features, $P(y)$ is the prior probability of the class label ('good or 'bad'), and $P(x_1, x_2, \\dots, x_m \\mid y)$ is the likelihood of the wine features given the class label. The denominator $P(x_1, x_2, \\dots, x_m)$ is a normalization constant that ensures that the probabilities sum to 1.\n",
    "\n",
    "**Once the model is trained, it seeks to classify new wine by calculating the probability of the bottle being 'good' or 'bad' given the chemical input features and selecting the class with the highest calculated probability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7f0d0-1a04-47c3-b0ee-d6beff570e53",
   "metadata": {},
   "source": [
    "![title](results/bayesreport.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef249567-5ee1-4661-a25d-115bce210fa0",
   "metadata": {},
   "source": [
    "Naive Bayes, similar to our Decision Tree, also performed marginally worse than our first two models. This could be due to both of these models:\n",
    "\n",
    "- Not taking into account interdependence between variables\n",
    "- The lack of scaling\n",
    "- General simplicity of each model compared to SVM RBF and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcb3ee-fac1-457b-834c-2d42d3274e5a",
   "metadata": {},
   "source": [
    "![title](results/bayescorrect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01579a4a-1b1d-46a0-bd1a-69b75c259a67",
   "metadata": {},
   "source": [
    "## Comparing Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41a229-d933-49a9-aeaa-46f83a8f3c48",
   "metadata": {},
   "source": [
    "![title](results/scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ee361-56b3-4d28-b366-20ecf4140b51",
   "metadata": {},
   "source": [
    "*Figure 4: Comparing 'accuracy' metric of each model - LogisticRegression, SVM RBF, Naive Bayes, and DecisionTreeClassifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bba95-6a02-4fa2-bb28-bafa1143204b",
   "metadata": {},
   "source": [
    "After training and testing the models, the **SVC RBF** achieved the highest test score accuracy score of 0.76. Furthermore, all four models performed significantly better than our 'most-frequent' baseline strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea76eb9-66a1-4b97-9682-6f61973eaf1a",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78a3d4-4026-42aa-9dfe-81080641a5d5",
   "metadata": {},
   "source": [
    "Our trained models all performed better than the baseline model, but a best score of 0.76 is not sufficient to accurately predict wine quality based on its chemical qualities. While the lower accuracy score is not ideal, it is expected. Errors in the predictions may be due to the limitations of the methods used; however, it is likely also due to the varying preferences of the individuals surveyed to obtain the data. Limitations of the models used have been previously mentioned in the methods section, so this section will focus on human biases. The standards and qualities of wine are all determined by people and even though there may be experts that know the field, quality is still a subjective measurement and experts will still have bias in their judgments. As noted in the paper by Cortez et al, sensory evaluations and analyses are performed on the wine tasters during wine certification in Portugal; however, even then, it does not fully account for personal preferences or biases. As such, it is difficult to account or correct for the subjectivity of human taste. This also means that it will be difficult for an algorithm to predict the quality of wine since patterns may not always be present or clear. Therefore, a lower accuracy score is expected.\n",
    "\n",
    "Another limitation in this method of predicting quality of wine based on chemical features is the fact that this data set only considers 11 features. In a 1978 study on wine quality, there were statistically significant correlations between the amounts of pigments in the wine and the perceived quality of the wine (Jackson, et al., 1978). Pigments were not considered in the data set used in this project. These pigments may have some effect on the flavour but can also influence the appearance. It was not apparent whether or not the wine data was from a blind tasting or if visual indicators of quality were also considered, so there are additional factors that were not included within the analysis.\n",
    "\n",
    "Although there are several limitations to the methods used in this project, there may still be use for machine learning algorithms within the realm of predictions of wine quality. Primarily, it could still be useful in more targeted marketing techniques or finding ideal wines for more specific consumer groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941a24b-279a-4812-97d4-a2648572f368",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b4544-754b-4d96-835e-48f394c47d72",
   "metadata": {},
   "source": [
    "1. Filipe-Ribeiro, L., Milheiro, J., Matos, C. C., Cosme, F., & Nunes, F. M. (2017). Reduction of 4-ethylphenol and 4-ethylguaiacol in red wine by activated carbons with different physicochemical characteristics: Impact on wine quality. Food Chemistry, 229(C), 242-251. https://doi.org/10.1016/j.foodchem.2017.02.066\n",
    "\n",
    "2. Hannah Ritchie and Max Roser (2018) - \"Alcohol Consumption\". Published online at OurWorldInData.org. Retrieved from: 'https://ourworldindata.org/alcohol-consumption' [Online Resource]\n",
    "\n",
    "3. Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009) Modeling wine preferences by data mining from physicochemical properties. Decision Support System 547-553. https://doi.org/10.1016/j.dss.2009.05.016\n",
    "\n",
    "4. Jackson, J. G., Timberlake, C. F., Bridle, P., & Vallis, L. (1978) Red wine quality: Correlations between colour, aroma and flavour and pigment and other parameters of young Beaujolais. Journal of the Science of Food and Agriculture, 715-727. https://doi.org/10.1002/jsfa.2740290810\n",
    "\n",
    "\n",
    "\n",
    "Dataset:. P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
